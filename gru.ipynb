{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Introduction\n",
    "\n",
    "2. Foundations of GRUs\n",
    "- Detailed explanation of RNNs and their limitations.\n",
    "- Introduction to the architecture and mechanisms of GRUs.\n",
    "- Comparison of GRUs with other recurrent units like LSTM (Long Short-Term Memory).\n",
    "- Math stuff\n",
    "\n",
    "3. Architectural Design and Variants\n",
    "- Backpropagation thourgh time\n",
    "- In-depth analysis of the GRU architecture, including the reset gate and update gate.\n",
    "- Exploration of different variants and modifications of GRUs proposed in the literature.\n",
    "- Discussion on parameter tuning and hyperparameter settings.\n",
    "\n",
    "4. Applications of GRUs\n",
    "- Review of real-world applications of GRUs, including natural language processing, speech recognition, and time series forecasting.\n",
    "- Case studies highlighting successful implementations of GRU-based models.\n",
    "\n",
    "5. Advancements and Research Trends\n",
    "- Overview of recent advancements and breakthroughs in GRU research.\n",
    "- Discussion on open research challenges and emerging trends in sequence modeling.\n",
    "\n",
    "6. Conclusion\n",
    "- Summary of key findings and contributions.\n",
    "- Recommendations for future research directions and potential improvements in GRU architectures.\n",
    "\n",
    "7. References\n",
    "- Citations of relevant research papers and resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction ()\n",
    "\n",
    "### RNN\n",
    "Modeling and predicting sequential data requires a different approach from standard regression or classification. Luckily, a particular type of Neural Networks called Recurrent Neural Networks (RNNs) are specifically designed for that purpose.\n",
    "\n",
    "`toggle`\n",
    "\n",
    "A recurrent neural network (RNN) is the type of artificial neural network (ANN) that is used to address the limitations of traditional neural networks, when it comes to processing sequential data. Traditional approaches to Neural Network Architecture posses a significant drawback, due to which it is unable to handle sequential data effectively and capture the dependencies between inputs. RNN remembers past inputs due to an internal memory which is useful for predicting target values.\n",
    "\n",
    "`toggle`\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <td><img src=\"./images/FNN.png\" alt=\"Image 1\"></td>\n",
    "    <td><img src=\"./images/RNN.png\" alt=\"Image 2\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Forward Neural Network</td>\n",
    "    <td>Reccurent Neural Network</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "__Important characteristics:__\n",
    "1. RNN shares the same weights within each layer of the network.\n",
    "2. RNN doesn't take into consideration just the actual input but also the previous inputs which allows it to memorize what happens previously. \n",
    "\n",
    "Simple RNN models usually run into two major issues. These issues are related to gradient, which is the slope of the loss function along with the error function.\n",
    "\n",
    "- Vanishing Gradient problem occurs when the gradient becomes so small that updating parameters becomes insignificant; eventually the algorithm stops learning.\n",
    "    \n",
    "- Exploding Gradient problem occurs when the gradient becomes too large, which makes the model unstable. In this case, larger error gradients accumulate, and the model weights become too large. This issue can cause longer training times and poor model performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRU (Gated recurrent unit)\n",
    "Introduced by Cho(`add wiki ink`), et al. in 2014(`add original paper link #1`), it aims to solve the __vanishing gradient problem__ which comes with a standard recurrent neural network. GRU can also be considered as a variation on the LSTM because both are designed similarly and, in some cases, produce equally excellent results.<br>\n",
    "\n",
    "The main idea behind GRU is to use gating mechanisms to selectively update the hidden state of the network at each time step. The gating mechanisms are used to control the flow of information in and out of the network. The GRU has two gating mechanisms, called the reset gate and the update gate.\n",
    "\n",
    "The reset gate determines how much of the previous hidden state should be forgotten, while the update gate determines how much of the new input should be used to update the hidden state. The output of the GRU is calculated based on the updated hidden state.\n",
    "\n",
    "`4 quiz questions`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Foundation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM VS GRU \n",
    "`3 quiz questions`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Architecture\n",
    "<center><img src=\"GRU_arch.gif\" alt=\"GRU architecture\" width=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reset Gate and Update Gate\n",
    "\n",
    "In GRU we have two gates the reset gate and the update gate. The outputs of the gates are given by two fully connected layers with a [sigmoid activation <img src=\"sigma.svg\" width=\"25\"/> function](https://fedmug.github.io/kbtu-ml-book/mlp/activations.html).\n",
    "<center><img src=\"Reset gate and Update gate.svg\" alt=\"Reset gate and Update gate\" width=\"500\"/></center>\n",
    "\n",
    "Suppose that the <b>input</b> is minibatch $X_t \\in \\mathbb{R}^{n \\times d}$ (where $n$ is <b>number of examples</b>, $d$ is <b>number of inputs</b> and $t$ is <b>time step</b>).\n",
    "\n",
    "The <b>hidden state</b> of the previous time step is $H_{t-1} \\in \\mathbb{R}^{n \\times h}$ (where $h$ is <b>number of hidden units</b>).\n",
    "\n",
    "Then the <b>reset gate</b> $ R_t \\in \\mathbb{R}^{n \\times h}$ and <b>update gate</b> $ Z_t \\in \\mathbb{R}^{n \\times h}$.\n",
    "\n",
    "They are computed as follows:\n",
    "\n",
    "\\begin{align}\n",
    "    R_t &= \\sigma(X_t W_{r} + H_{t-1} U_{r} + b_r) \\\\\n",
    "    Z_t &= \\sigma(X_t W_{z} + H_{t-1} U_{z} + b_z) \\\\\n",
    "\\end{align}\n",
    "\n",
    "where $W_{r}, W_{z} \\in \\mathbb{R}^{d \\times h}$ and $U_{r},U_{z} \\in \\mathbb{R}^{h \\times h}$ are weight parameters,\n",
    "\n",
    "$b_r, b_z \\in \\mathbb{R}^{1 \\times h}$ are bias parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Candidate Hidden State\n",
    "\n",
    "Next, we combine the <b>reset gate</b> $R_t$ with the standard updating mechanism, resulting in a <b>candidate hidden state</b> $\\tilde{H}_t$ at time step $t$. Here we use a [tanh activation <img src=\"tanh.svg\" width=\"25\"/> function](https://fedmug.github.io/kbtu-ml-book/mlp/activations.html):\n",
    "\n",
    "\\begin{align}\n",
    "\\tilde{H}_t &= \\tanh(X_t W_{h} + (R_t \\odot H_{t-1}) U_{h} + b_h) \\\\\n",
    "\\end{align}\n",
    "\n",
    "where $W_{h} \\in \\mathbb{R}^{d \\times h}$ and $U_{h} \\in \\mathbb{R}^{h \\times h}$ are weight parameters,\n",
    "\n",
    "$b_h \\in \\mathbb{R}^{1 \\times h}$ is the bias. \n",
    "\n",
    "$\\odot$ - [Hadamard (elementwise) product](https://en.wikipedia.org/wiki/Hadamard_product_(matrices)) operator.\n",
    "\n",
    "<center><img src=\"Candidate hidden state.svg\" alt=\"Candidate hidden state\" width=\"500\"/></center>\n",
    "\n",
    "If the reset gate $R_t$ are close to 1, the model acts like a regular [Recurrent Neural Network (RNN)](https://fedmug.github.io/kbtu-ml-book/rnn/vanilla_rnn.html). Conversely, when the values in $R_t$ are close to 0, the candidate hidden state is computed using a [Multi-Layer Perceptron (MLP)](https://fedmug.github.io/kbtu-ml-book/mlp/mlp.html) with the current input $X_t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hidden State\n",
    "\n",
    "Finally, we need to incorporate the effect of the update gate $Z_t$. This determines how much the new hidden state $H_t$ matches the old state $H_{t-1}$ compared to how much it resembles the new candidate state $H_t$. The update gate $Z_t$ can be used for this purpose by taking elementwise convex combinations of $H_{t-1}$ and $H_t$. This leads to the final update equation for the GRU.\n",
    "\n",
    "\\begin{align}\n",
    "H_t &= (1-Z_t) \\odot H_{t-1} + Z_t \\odot  \\tilde{H}_t \n",
    "\\end{align}\n",
    "\n",
    "<center><img src=\"hidden state.svg\" alt=\"hidden state\" width=\"500\"/></center>\n",
    "\n",
    "When $Z_t$ is close to 1, we keep the old state, ignoring the information from the current input $X_t$ and effectively skipping the current time step in the dependency chain. \n",
    "\n",
    "On the other hand, when $Z_t$ is close to 0, the new latent state $H_t$ approaches the candidate latent state $\\tilde{H}_t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> GRUs possess two distinctive characteristics:\n",
    ">\n",
    "> - Reset gates are employed to capture short-term dependencies within sequences.\n",
    "> \n",
    "> - Update gates are utilized to capture long-term dependencies within sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "1. https://arxiv.org/pdf/1412.3555v1.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
