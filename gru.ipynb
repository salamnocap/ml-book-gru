{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Introduction\n",
    "\n",
    "2. Foundations of GRUs\n",
    "- Detailed explanation of RNNs and their limitations.\n",
    "- Introduction to the architecture and mechanisms of GRUs.\n",
    "- Comparison of GRUs with other recurrent units like LSTM (Long Short-Term Memory).\n",
    "- Math stuff\n",
    "\n",
    "3. Architectural Design and Variants\n",
    "- In-depth analysis of the GRU architecture, including the reset gate and update gate.\n",
    "- Exploration of different variants and modifications of GRUs proposed in the literature.\n",
    "- Discussion on parameter tuning and hyperparameter settings.\n",
    "\n",
    "4. Applications of GRUs\n",
    "- Review of real-world applications of GRUs, including natural language processing, speech recognition, and time series forecasting.\n",
    "- Case studies highlighting successful implementations of GRU-based models.\n",
    "\n",
    "5. Advancements and Research Trends\n",
    "- Overview of recent advancements and breakthroughs in GRU research.\n",
    "- Discussion on open research challenges and emerging trends in sequence modeling.\n",
    "\n",
    "6. Conclusion\n",
    "- Summary of key findings and contributions.\n",
    "- Recommendations for future research directions and potential improvements in GRU architectures.\n",
    "\n",
    "7. References\n",
    "- Citations of relevant research papers and resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "\n",
    "### RNN\n",
    "Modeling and predicting sequential data requires a different approach from standard regression or classification. Luckily, a particular type of Neural Networks called Recurrent Neural Networks (RNNs) are specifically designed for that purpose.\n",
    "\n",
    "`toggle`\n",
    "\n",
    "A recurrent neural network (RNN) is the type of artificial neural network (ANN) that is used to address the limitations of traditional neural networks, when it comes to processing sequential data. Traditional approaches to Neural Network Architecture posses a significant drawback, due to which it is unable to handle sequential data effectively and capture the dependencies between inputs. RNN remembers past inputs due to an internal memory which is useful for predicting target values.\n",
    "\n",
    "`toggle`\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <td><img src=\"images/Feed_forward_neural_network.svg\" alt=\"Image 1\" width=\"400\"></td>\n",
    "    <td><img src=\"images/Recurrent_Neural_Network.svg\" alt=\"Image 2\" width=\"400\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Feed Forward Neural Network</td>\n",
    "    <td>Reccurent Neural Network</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "__Important characteristics:__\n",
    "1. RNN shares the same weights within each layer of the network.\n",
    "2. RNN doesn't take into consideration just the actual input but also the previous inputs which allows it to memorize what happens previously. \n",
    "\n",
    "Simple RNN models usually run into two major issues. These issues are related to gradient, which is the slope of the loss function along with the error function.\n",
    "\n",
    "- Vanishing Gradient problem occurs when the gradient becomes so small that updating parameters becomes insignificant; eventually the algorithm stops learning.\n",
    "    \n",
    "- Exploding Gradient problem occurs when the gradient becomes too large, which makes the model unstable. In this case, larger error gradients accumulate, and the model weights become too large. This issue can cause longer training times and poor model performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRU (Gated recurrent unit)\n",
    "Introduced by Cho(`add wiki ink`), et al. in 2014(`add original paper link #1`), it aims to solve the __vanishing gradient problem__ which comes with a standard recurrent neural network. GRU can also be considered as a variation on the LSTM because both are designed similarly and, in some cases, produce equally excellent results.<br>\n",
    "\n",
    "The main idea behind GRU is to use gating mechanisms to selectively update the hidden state of the network at each time step. The gating mechanisms are used to control the flow of information in and out of the network. The GRU has two gating mechanisms, called the reset gate and the update gate.\n",
    "\n",
    "The reset gate determines how much of the previous hidden state should be forgotten, while the update gate determines how much of the new input should be used to update the hidden state. The output of the GRU is calculated based on the updated hidden state.\n",
    "\n",
    "`2 quiz questions`\n",
    "\n",
    "### HOW IT WORKS\n",
    "\n",
    "Typical RNN structure contains input, hidden and output layers. Note that we can have any number of nodes, and the example below is just for illustration.\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <td><img src=\"./images/RNN_Architecture.png\" alt=\"Image 1\" width=\"500\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Typical RNN structure</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "RNNs utilize recurrent units to learn from the sequence data, which is true for all three types — standard RNN, LSTM, and GRU.\n",
    "The main differences between models lie inside this recurrent units.\n",
    "For example, standard RNN uses a hidden state to remember information. \n",
    "Meanwhile, LSTM and GRU introduce gates to control what to remember and what to forget before updating the hidden state. \n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <td><img src=\"./images/VanillaVsLSTM.png\" alt=\"Image 1\" width=\"700\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Typical RNN structure</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HOW GRU Works\n",
    "GRU is similar to LSTM, but it has fewer gates. Also, it relies solely on a hidden state for memory transfer between recurrent units, so there is no separate cell state. Let’s analyze this simplified GRU diagram in detail (weights and biases not shown)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GATES\n",
    "As it was already mentioned above GRU attempts to solve the vanishing gradient problem of a standard RNN. GRU uses, so-called, update gate and reset gate. Basically, these are two vectors which decide what information should be passed to the output. The special thing about them is that they can be trained to keep information from long ago, without washing it through time or remove information which is irrelevant to the prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Foundation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM VS GRU \n",
    "`3 quiz questions`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic idea of using a gating mechanism to learn long-term dependencies is the same as in a LSTM, but there are a few key differences:\n",
    "\n",
    "* A GRU has two gates, an LSTM has three gates. In LSTM they are the Input gate, Forget gate, and Output gate. Whereas in GRU we have a Reset gate and Update gate.\n",
    "\n",
    "* GRU doesn’t possess an internal memory that is different from the exposed hidden state.\n",
    "\n",
    "* The input and forget gates are coupled by an update gate z and the reset gate r is applied directly to the previous hidden state. Thus, the responsibility of the reset gate in a LSTM is really split up into both r and z.\n",
    "\n",
    "* We don’t apply a second nonlinearity when computing the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There isn’t a clear winner which one is better. \n",
    "In many tasks both architectures show comparable performance and changing parameters like layer size is can be more important than picking the ideal architecture. \n",
    "GRU has fewer parameters (U and W are smaller) and thus may train a bit faster or need less data to generalize. On the other hand, if you have enough data, the LSTM may provide better results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we set the reset to all 1’s and  update gate to all 0’s we again arrive at our plain RNN model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Architecture\n",
    "<center><img src=\"images/GRU_arch.gif\" alt=\"GRU architecture\" width=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reset Gate and Update Gate\n",
    "\n",
    "In GRU we have two gates the reset gate and the update gate. The outputs of the gates are given by two fully connected layers with a [sigmoid activation <img src=\"images/sigma.svg\" width=\"25\"/> function](https://fedmug.github.io/kbtu-ml-book/mlp/activations.html).\n",
    "<center><img src=\"images/Reset gate and Update gate.svg\" alt=\"Reset gate and Update gate\" width=\"500\"/></center>\n",
    "\n",
    "Suppose that the <b>input</b> is minibatch $X_t \\in \\mathbb{R}^{n \\times d}$ (where $n$ is <b>number of examples</b>, $d$ is <b>number of inputs</b> and $t$ is <b>time step</b>).\n",
    "\n",
    "The <b>hidden state</b> of the previous time step is $H_{t-1} \\in \\mathbb{R}^{n \\times h}$ (where $h$ is <b>number of hidden units</b>).\n",
    "\n",
    "Then the <b>reset gate</b> $ R_t \\in \\mathbb{R}^{n \\times h}$ and <b>update gate</b> $ Z_t \\in \\mathbb{R}^{n \\times h}$.\n",
    "\n",
    "They are computed as follows:\n",
    "\n",
    "\\begin{align}\n",
    "    R_t &= \\sigma(X_t W_{r} + H_{t-1} U_{r} + b_r) \\\\\n",
    "    Z_t &= \\sigma(X_t W_{z} + H_{t-1} U_{z} + b_z) \\\\\n",
    "\\end{align}\n",
    "\n",
    "where $W_{r}, W_{z} \\in \\mathbb{R}^{d \\times h}$ and $U_{r},U_{z} \\in \\mathbb{R}^{h \\times h}$ are weight parameters,\n",
    "\n",
    "$b_r, b_z \\in \\mathbb{R}^{1 \\times h}$ are bias parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Candidate Hidden State\n",
    "\n",
    "Next, we combine the <b>reset gate</b> $R_t$ with the standard updating mechanism, resulting in a <b>candidate hidden state</b> $\\tilde{H}_t$ at time step $t$. Here we use a [tanh activation <img src=\"images/tanh.svg\" width=\"25\"/> function](https://fedmug.github.io/kbtu-ml-book/mlp/activations.html):\n",
    "\n",
    "\\begin{align}\n",
    "\\tilde{H}_t &= \\tanh(X_t W_{h} + (R_t \\odot H_{t-1}) U_{h} + b_h) \\\\\n",
    "\\end{align}\n",
    "\n",
    "where $W_{h} \\in \\mathbb{R}^{d \\times h}$ and $U_{h} \\in \\mathbb{R}^{h \\times h}$ are weight parameters,\n",
    "\n",
    "$b_h \\in \\mathbb{R}^{1 \\times h}$ is the bias. \n",
    "\n",
    "$\\odot$ - [Hadamard (elementwise) product](https://en.wikipedia.org/wiki/Hadamard_product_(matrices)) operator.\n",
    "\n",
    "<center><img src=\"images/Candidate hidden state.svg\" alt=\"Candidate hidden state\" width=\"500\"/></center>\n",
    "\n",
    "If the reset gate $R_t$ are close to 1, the model acts like a regular [Recurrent Neural Network (RNN)](https://fedmug.github.io/kbtu-ml-book/rnn/vanilla_rnn.html). Conversely, when the values in $R_t$ are close to 0, the candidate hidden state is computed using a [Multi-Layer Perceptron (MLP)](https://fedmug.github.io/kbtu-ml-book/mlp/mlp.html) with the current input $X_t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hidden State\n",
    "\n",
    "Finally, we need to incorporate the effect of the update gate $Z_t$. This determines how much the new hidden state $H_t$ matches the old state $H_{t-1}$ compared to how much it resembles the new candidate state $H_t$. The update gate $Z_t$ can be used for this purpose by taking elementwise convex combinations of $H_{t-1}$ and $H_t$. This leads to the final update equation for the GRU.\n",
    "\n",
    "\\begin{align}\n",
    "H_t &= (1-Z_t) \\odot H_{t-1} + Z_t \\odot  \\tilde{H}_t \n",
    "\\end{align}\n",
    "\n",
    "<center><img src=\"images/hidden state.svg\" alt=\"hidden state\" width=\"500\"/></center>\n",
    "\n",
    "When $Z_t$ is close to 1, we keep the old state, ignoring the information from the current input $X_t$ and effectively skipping the current time step in the dependency chain. \n",
    "\n",
    "On the other hand, when $Z_t$ is close to 0, the new latent state $H_t$ approaches the candidate latent state $\\tilde{H}_t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> GRUs possess two distinctive characteristics:\n",
    ">\n",
    "> - Reset gates are employed to capture short-term dependencies within sequences.\n",
    "> \n",
    "> - Update gates are utilized to capture long-term dependencies within sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modifications:\n",
    "\n",
    "### Minimal gated unit\n",
    " \n",
    "* similar to the fully gated unit, but the update and reset gate vector is merged into a forget gate. \n",
    "\\begin{align}\n",
    "    f_t &= \\sigma(W_f [h_{t-1}, x_t] + b_f) \\\\\n",
    "    \\tilde{h}_t &= \\tanh(W_h [f_t \\odot h_{t-1}, x_t] + b_h)\\\\\n",
    "    h_t &= (1 - f_t) \\odot h_{t-1} + f_t \\odot \\tilde{h}_t\n",
    "\\end{align}\n",
    "\n",
    "### Light gated recurrent unit\n",
    "\n",
    "* removes the reset gate altogether\n",
    "* replaces tanh with the ReLU activation \n",
    "* applies batch normalization (BN)\n",
    "\n",
    "\\begin{align}\n",
    "z_t &= \\sigma\\left(\\text{BN}\\left(W_z x_t\\right) + U_z h_{t-1}\\right)  \\\\\n",
    "\\tilde{h}_t &= \\text{ReLU}\\left(\\text{BN}\\left(W_h x_t\\right) + U_h h_{t-1}\\right) \\\\\n",
    "h_t &= z_t \\odot h_{t-1} + (1 - z_t) \\odot \\tilde{h}_t\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Applications of GRUs\n",
    "\n",
    "add examples of gru applied in real world\n",
    "\n",
    "4 quizez"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "1. https://arxiv.org/pdf/1412.3555v1.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MATH \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$z_t=\\sigma (W_z x_t + U_z h_{t-1} + b_z)$$\n",
    "$$r_t=\\sigma (W_r x_t + U_r h_{t-1} + b_r)$$\n",
    "$$\\hat{h_t}=\\sigma (W_h x_t + U_h(r_t ))$$\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
