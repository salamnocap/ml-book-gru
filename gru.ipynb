{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Introduction\n",
    "- Background and motivation for studying GRU architectures.\n",
    "- Overview of the importance of sequence modeling in machine learning and AI.\n",
    "- Research objectives and scope.\n",
    "\n",
    "2. Foundations of GRUs\n",
    "- Detailed explanation of RNNs and their limitations.\n",
    "- Introduction to the architecture and mechanisms of GRUs.\n",
    "- Comparison of GRUs with other recurrent units like LSTM (Long Short-Term Memory).\n",
    "\n",
    "3. Architectural Design and Variants\n",
    "- In-depth analysis of the GRU architecture, including the reset gate and update gate.\n",
    "- Exploration of different variants and modifications of GRUs proposed in the literature.\n",
    "- Discussion on parameter tuning and hyperparameter settings.\n",
    "\n",
    "4. Applications of GRUs\n",
    "- Review of real-world applications of GRUs, including natural language processing, speech recognition, and time series forecasting.\n",
    "- Case studies highlighting successful implementations of GRU-based models.\n",
    "\n",
    "5. Advancements and Research Trends\n",
    "- Overview of recent advancements and breakthroughs in GRU research.\n",
    "- Discussion on open research challenges and emerging trends in sequence modeling.\n",
    "\n",
    "6. Empirical Evaluation\n",
    "- Presentation of experimental results on benchmark datasets to evaluate the performance of GRU-based models.\n",
    "- Comparison of GRUs with other relevant architectures.\n",
    "\n",
    "7. Discussion and Analysis\n",
    "- Interpretation of experimental findings and their implications.\n",
    "- Discussion on the scalability, interpretability, and generalizability of GRU models.\n",
    "\n",
    "8. Conclusion\n",
    "- Summary of key findings and contributions.\n",
    "- Recommendations for future research directions and potential improvements in GRU architectures.\n",
    "\n",
    "9. References\n",
    "- Citations of relevant research papers and resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gated Recurrent Unit (GRU)** is a type of [Recurrent Neural Network (RNN)](https://fedmug.github.io/kbtu-ml-book/rnn/vanilla_rnn.html) architecture that was introduced in 2014 by [Kyunghyun Cho et al](https://arxiv.org/pdf/1412.3555.pdf). It is similar to [Long Short-Term Memory (LSTM)](https://fedmug.github.io/kbtu-ml-book/rnn/lstm.html) in that it can capture long-term dependencies in sequential data, but it has fewer parameters and is faster to train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recurrent neural networks work differently compared to convolutional. In these networks we have layers with neurons that not only connect to the neurons next layer but also to neurons of the previous or of their own layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"GRU_arch.gif\" alt=\"GRU architecture\" width=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reset Gate and Update Gate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In GRU we have two gates the reset gate and the update gate. The outputs of the gates are given by two fully connected layers with a [sigmoid activation <img src=\"sigma.svg\" width=\"25\"/> function](https://fedmug.github.io/kbtu-ml-book/mlp/activations.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"Reset gate and Update gate.svg\" alt=\"Reset gate and Update gate\" width=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Suppose that the <b>input</b> is minibatch $X_t \\in \\mathbb{R}^{n \\times d}$ (where $n$ is <b>number of examples</b>, $d$ is <b>number of inputs</b> and $t$ is <b>time step</b>).\n",
    ">\n",
    ">The <b>hidden state</b> of the previous time step is $H_{t-1} \\in \\mathbb{R}^{n \\times h}$ (where $h$ is <b>number of hidden units</b>).\n",
    ">\n",
    ">Then the <b>reset gate</b> $ R_t \\in \\mathbb{R}^{n \\times h}$ and <b>update gate</b> $ Z_t \\in \\mathbb{R}^{n \\times h}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They are computed as follows:\n",
    "\n",
    "\\begin{align}\n",
    "    R_t &= \\sigma(X_t W_{r} + H_{t-1} U_{r} + b_r) \\\\\n",
    "    Z_t &= \\sigma(X_t W_{z} + H_{t-1} U_{z} + b_z) \\\\\n",
    "\\end{align}\n",
    "\n",
    "where $W_{r}, W_{z} \\in \\mathbb{R}^{d \\times h}$ and $U_{r},U_{z} \\in \\mathbb{R}^{h \\times h}$ are weight parameters,\n",
    "\n",
    "$b_r, b_z \\in \\mathbb{R}^{1 \\times h}$ are bias parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Candidate Hidden State"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we combine the <b>reset gate</b> $R_t$ with the standard updating mechanism, resulting in a <b>candidate hidden state</b> $\\tilde{H}_t$ at time step $t$. Here we use a [tanh activation <img src=\"tanh.svg\" width=\"25\"/> function](https://fedmug.github.io/kbtu-ml-book/mlp/activations.html):\n",
    "\n",
    "\\begin{align}\n",
    "\\tilde{H}_t &= \\tanh(X_t W_{h} + (R_t \\odot H_{t-1}) U_{h} + b_h) \\\\\n",
    "\\end{align}\n",
    "\n",
    "where $W_{h} \\in \\mathbb{R}^{d \\times h}$ and $U_{h} \\in \\mathbb{R}^{h \\times h}$ are weight parameters,\n",
    "\n",
    "$b_h \\in \\mathbb{R}^{1 \\times h}$ is the bias. \n",
    "\n",
    "$\\odot$ - [Hadamard (elementwise) product](https://en.wikipedia.org/wiki/Hadamard_product_(matrices)) operator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"Hidden state.svg\" alt=\"Hidden state\" width=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> If the reset gate $R_t$ are close to 1, the model acts like a regular [Recurrent Neural Network (RNN)](https://fedmug.github.io/kbtu-ml-book/rnn/vanilla_rnn.html). Conversely, when the values in $R_t$ are close to 0, the candidate hidden state is computed using a [Multi-Layer Perceptron (MLP)](https://fedmug.github.io/kbtu-ml-book/mlp/mlp.html) with the current input $X_t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hidden State"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need to incorporate the effect of the update gate $Z_t$. This determines how much the new hidden state $H_t$ matches the old state $H_{t-1}$ compared to how much it resembles the new candidate state $H_t$. The update gate $Z_t$ can be used for this purpose by taking elementwise convex combinations of $H_{t-1}$ and $H_t$. This leads to the final update equation for the GRU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "H_t &= (1-Z_t) \\odot H_{t-1} + Z_t \\odot  \\tilde{H}_t \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"Candidate hidden state.svg\" alt=\" Candidate hidden state\" width=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">When $Z_t$ is close to 1, we keep the old state, ignoring the information from the current input $X_t$ and effectively skipping the current time step in the dependency chain. \n",
    ">\n",
    ">On the other hand, when $Z_t$ is close to 0, the new latent state $H_t$ approaches the candidate latent state $\\tilde{H}_t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> GRUs possess two distinctive characteristics:\n",
    ">\n",
    "> - Reset gates are employed to capture short-term dependencies within sequences.\n",
    "> \n",
    "> - Update gates are utilized to capture long-term dependencies within sequences."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
