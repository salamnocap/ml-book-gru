{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gated Recurrent Unit (GRU)** is a type of [Recurrent Neural Network (RNN)](https://fedmug.github.io/kbtu-ml-book/rnn/vanilla_rnn.html) architecture that was introduced in 2014 by [Kyunghyun Cho et al](https://arxiv.org/pdf/1412.3555.pdf). It is similar to [Long Short-Term Memory (LSTM)](https://fedmug.github.io/kbtu-ml-book/rnn/lstm.html) in that it can capture long-term dependencies in sequential data, but it has fewer parameters and is faster to train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"GRU_arch.gif\" alt=\"GRU architecture\" width=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reset Gate and Update Gate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In GRU we have two gates the reset gate and the update gate. The outputs of the gates are given by two fully connected layers with a [sigmoid activation <img src=\"sigma.svg\" width=\"25\"/> function](https://fedmug.github.io/kbtu-ml-book/mlp/activations.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"Reset gate and Update gate.svg\" alt=\"Reset gate and Update gate\" width=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Suppose that the <b>input</b> is minibatch $X_t \\in \\mathbb{R}^{n \\times d}$ (where $n$ is <b>number of examples</b>, $d$ is <b>number of inputs</b> and $t$ is <b>time step</b>).\n",
    ">\n",
    ">The <b>hidden state</b> of the previous time step is $H_{t-1} \\in \\mathbb{R}^{n \\times h}$ (where $h$ is <b>number of hidden units</b>).\n",
    ">\n",
    ">Then the <b>reset gate</b> $ R_t \\in \\mathbb{R}^{n \\times h}$ and <b>update gate</b> $ Z_t \\in \\mathbb{R}^{n \\times h}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They are computed as follows:\n",
    "\n",
    "\\begin{align}\n",
    "    R_t &= \\sigma(X_t W_{r} + H_{t-1} U_{r} + b_r) \\\\\n",
    "    Z_t &= \\sigma(X_t W_{z} + H_{t-1} U_{z} + b_z) \\\\\n",
    "\\end{align}\n",
    "\n",
    "where $W_{r}, W_{z} \\in \\mathbb{R}^{d \\times h}$ and $U_{r},U_{z} \\in \\mathbb{R}^{h \\times h}$ are weight parameters,\n",
    "\n",
    "$b_r, b_z \\in \\mathbb{R}^{1 \\times h}$ are bias parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Candidate Hidden State"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we combine the <b>reset gate</b> $R_t$ with the standard updating mechanism, resulting in a <b>candidate hidden state</b> $\\tilde{H}_t$ at time step $t$. Here we use a [tanh activation <img src=\"tanh.svg\" width=\"25\"/> function](https://fedmug.github.io/kbtu-ml-book/mlp/activations.html):\n",
    "\n",
    "\\begin{align}\n",
    "\\tilde{H}_t &= \\tanh(X_t W_{h} + (R_t \\odot H_{t-1}) U_{h} + b_h) \\\\\n",
    "\\end{align}\n",
    "\n",
    "where $W_{h} \\in \\mathbb{R}^{d \\times h}$ and $U_{h} \\in \\mathbb{R}^{h \\times h}$ are weight parameters,\n",
    "\n",
    "$b_h \\in \\mathbb{R}^{1 \\times h}$ is the bias. \n",
    "\n",
    "$\\odot$ - [Hadamard (elementwise) product](https://en.wikipedia.org/wiki/Hadamard_product_(matrices)) operator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"Hidden state.svg\" alt=\"Hidden state\" width=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> If the reset gate $R_t$ are close to 1, the model acts like a regular [Recurrent Neural Network (RNN)](https://fedmug.github.io/kbtu-ml-book/rnn/vanilla_rnn.html). Conversely, when the values in $R_t$ are close to 0, the candidate hidden state is computed using a [Multi-Layer Perceptron (MLP)](https://fedmug.github.io/kbtu-ml-book/mlp/mlp.html) with the current input $X_t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hidden State"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need to incorporate the effect of the update gate $Z_t$. This determines how much the new hidden state $H_t$ matches the old state $H_{t-1}$ compared to how much it resembles the new candidate state $H_t$. The update gate $Z_t$ can be used for this purpose by taking elementwise convex combinations of $H_{t-1}$ and $H_t$. This leads to the final update equation for the GRU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "H_t &= (1-Z_t) \\odot H_{t-1} + Z_t \\odot  \\tilde{H}_t \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"Candidate hidden state.svg\" alt=\" Candidate hidden state\" width=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">When $Z_t$ is close to 1, we keep the old state, ignoring the information from the current input $X_t$ and effectively skipping the current time step in the dependency chain. \n",
    ">\n",
    ">On the other hand, when $Z_t$ is close to 0, the new latent state $H_t$ approaches the candidate latent state $\\tilde{H}_t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> GRUs possess two distinctive characteristics:\n",
    ">\n",
    "> - Reset gates are employed to capture short-term dependencies within sequences.\n",
    "> \n",
    "> - Update gates are utilized to capture long-term dependencies within sequences."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
