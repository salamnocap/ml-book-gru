{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91938327",
   "metadata": {},
   "source": [
    "# Naive Bayes classifier\n",
    "\n",
    "\n",
    "\n",
    "A Naive Bayes classifier is a probabilistic machine learning model that is widely used for classification tasks, particularly in natural language processing (NLP) and text classification, spam detection, sentiment analysis, and more. It is based on Bayes' theorem and makes a \"naive\" assumption about the independence of features. Despite its simplifying assumption, Naive Bayes often performs surprisingly well in practice.\n",
    "\n",
    "Here's an overview of the Naive Bayes classifier:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1277dd3f",
   "metadata": {},
   "source": [
    "1. <h4>Bayes' Theorem.</h4>\n",
    "\n",
    "Naive Bayes is based on Bayes' theorem, which is a fundamental theorem in probability theory. It relates the conditional probability of an event A given an event B to the conditional probability of event B given event A. In the context of classification:\n",
    "\n",
    "$$P(y|X) = \\frac{P(X)}{P(X|y) \\cdot P(y)}$$\n",
    "\n",
    "Where: \n",
    "\n",
    "* $P(y|X)$ is the probability of class y given the features X;\n",
    "* $P(X|y)$ is the likelihood, the probability of observing features X given class y;\n",
    "* $P(y)$ is the prior probability of class y;\n",
    "* $P(X)$ is the evidence, the probability of observing the features X.\n",
    "\n",
    "2. <h4>Conditional Independence</h4> \n",
    "\n",
    "The \"naive\" assumption made by Naive Bayes is that all features are conditionally independent given the class label. In other words, the presence or absence of one feature does not affect the presence or absence of any other feature. This simplifies the calculation of the likelihood, making the model computationally tractable.\n",
    "\n",
    "<h2>Types of Naive Bayes Classifiers:</h2>\n",
    "\n",
    "1. <h4>Multinomial Naive Bayes. </h4>\n",
    "This variant is commonly used for text classification, where features represent word counts or frequencies. It's suitable for tasks like document classification and spam detection.\n",
    "\n",
    "2. <h4>Gaussian Naive Bayes. </h4>\n",
    "It assumes that features follow a Gaussian (normal) distribution. It's used for continuous data, such as numerical measurements.\n",
    "\n",
    "3. <h4>Bernoulli Naive Bayes. </h4>\n",
    "This variant is designed for binary feature data, where features are either present (1) or absent (0). It's suitable for text data represented as binary vectors.\n",
    "\n",
    "\n",
    "<h4>Advantages</h4>\n",
    "\n",
    "* Simplicity and speed: Naive Bayes is computationally efficient and can handle high-dimensional data;\n",
    "* Works well with small datasets;\n",
    "* Often performs surprisingly well, especially in text classification tasks;\n",
    "* Provides probabilities, allowing for probabilistic predictions.\n",
    "\n",
    "<h4>Limitations</h4>\n",
    "\n",
    "* The \"naive\" assumption of feature independence may not hold in many real-world scenarios;\n",
    "* It can be sensitive to irrelevant features;\n",
    "* It doesn't handle missing data well.\n",
    "\n",
    "\n",
    "\n",
    "Naive Bayes classifiers are widely used as a baseline model in many classification problems due to their simplicity and effectiveness, especially when working with text data. However, they may not be the best choice for every problem, and more complex models like decision trees, random forests, or neural networks may provide better accuracy in some cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b23c5d8",
   "metadata": {},
   "source": [
    "### Python implementations of the models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e9b1ec",
   "metadata": {},
   "source": [
    "#####  Multinomial Naive Bayes\n",
    "Here's an example of how to implement a Multinomial Naive Bayes classifier in Python using scikit-learn. In this example, we'll use the famous \"20 Newsgroups\" dataset for text classification, where the task is to classify news articles into one of 20 different categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97c395a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.65\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "# Load the 20 Newsgroups dataset\n",
    "newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(newsgroups.data, newsgroups.target, test_size=0.25, random_state=42)\n",
    "\n",
    "# Create a pipeline for text classification with Multinomial Naive Bayes\n",
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer()),  # Convert text to a bag-of-words representation\n",
    "    ('tfidf', TfidfTransformer()),  # Convert raw frequency counts to TF-IDF values\n",
    "    ('clf', MultinomialNB())  # Multinomial Naive Bayes classifier\n",
    "])\n",
    "\n",
    "# Fit the model on the training data\n",
    "text_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = text_clf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba86898c",
   "metadata": {},
   "source": [
    "In this code:\n",
    "\n",
    "1. We load the 20 Newsgroups dataset and split it into training and testing sets.\n",
    "\n",
    "2. We create a pipeline for text classification, which includes text preprocessing steps like converting text to a bag-of-words representation and then to TF-IDF (Term Frequency-Inverse Document Frequency) values.\n",
    "\n",
    "3. We use the Multinomial Naive Bayes classifier from scikit-learn (MultinomialNB) as the final step in the pipeline.\n",
    "\n",
    "4. We fit the model on the training data and make predictions on the test data.\n",
    "\n",
    "5. Finally, we evaluate the model's accuracy on the test data.\n",
    "\n",
    "You can adapt this code to your own text classification problem by replacing the dataset and making any necessary adjustments to the text preprocessing steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeba0471",
   "metadata": {},
   "source": [
    "##### Gaussian Naive Bayes classifier\n",
    "\n",
    "Here's an example of how to implement a Gaussian Naive Bayes classifier in Python using scikit-learn. Gaussian Naive Bayes is typically used for classification problems where the features are continuous and assumed to follow a Gaussian (normal) distribution.\n",
    "\n",
    "In this example, we'll use the Iris dataset, which is a commonly used dataset for classification tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75687a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.98\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import metrics\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create a Gaussian Naive Bayes classifier\n",
    "gnb = GaussianNB()\n",
    "\n",
    "# Fit the model on the training data\n",
    "gnb.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = gnb.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468a81c8",
   "metadata": {},
   "source": [
    "In this code:\n",
    "\n",
    "1. We load the Iris dataset using load_iris from scikit-learn.\n",
    "\n",
    "2. We split the dataset into training and testing sets.\n",
    "\n",
    "3. We create a Gaussian Naive Bayes classifier using GaussianNB.\n",
    "\n",
    "4. We fit the model on the training data.\n",
    "\n",
    "5. We make predictions on the test data and evaluate the model's accuracy.\n",
    "\n",
    "This example demonstrates how to use Gaussian Naive Bayes for a simple classification problem. You can adapt this code to your own classification tasks with continuous features that are assumed to follow a Gaussian distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1064f867",
   "metadata": {},
   "source": [
    "##### Bernoulli Naive Bayes classifier\n",
    "\n",
    "\n",
    "Here's an example of how to implement a Bernoulli Naive Bayes classifier in Python using scikit-learn. Bernoulli Naive Bayes is typically used for binary classification tasks where features are binary, representing the presence or absence of certain attributes.\n",
    "\n",
    "In this example, we'll use a synthetic dataset for binary classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d21cafb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.80\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn import metrics\n",
    "\n",
    "# Generate a synthetic binary classification dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create a Bernoulli Naive Bayes classifier\n",
    "bnb = BernoulliNB()\n",
    "\n",
    "# Fit the model on the training data\n",
    "bnb.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = bnb.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9223e271",
   "metadata": {},
   "source": [
    "In this code:\n",
    "\n",
    "1. We generate a synthetic binary classification dataset using make_classification from scikit-learn. You can replace this with your own dataset.\n",
    "\n",
    "2. We split the dataset into training and testing sets.\n",
    "\n",
    "3. We create a Bernoulli Naive Bayes classifier using BernoulliNB.\n",
    "\n",
    "4. We fit the model on the training data.\n",
    "\n",
    "5. We make predictions on the test data and evaluate the model's accuracy.\n",
    "\n",
    "This example demonstrates how to use Bernoulli Naive Bayes for a binary classification problem with binary features. You can adapt this code to your own binary classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd39cfb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
