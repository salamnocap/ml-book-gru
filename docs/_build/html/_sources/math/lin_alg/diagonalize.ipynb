{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diagonalization\n",
    "\n",
    "У диагональной матрицы $\\boldsymbol \\Lambda = \\mathrm{diag}\\{\\lambda_1, \\ldots, \\lambda_n\\}$ всё просто с собственными числами и векторами. На главной диагонали у неё стоят собственные значения, а собственные векторы — это векторы из стандартного базиса в $\\mathbb R^n$:\n",
    "\n",
    "$$\n",
    "    \\boldsymbol \\Lambda e_i = \\lambda_i \\boldsymbol e_i, \\quad i = 1, \\ldots, n. \n",
    "$$\n",
    "\n",
    "Если матрицу $\\boldsymbol A \\in\\mathbb R^{n\\times n}$ можно привести к диагональному виду преобразованием подобия, то она называется  **диагонализируемой**. Диагонализация матрицы эквивалентна наличию базиса из собственных векторов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Теорема**. Если у матрицы $\\boldsymbol A \\in \\mathbb R^{n\\times n}$ есть $n$ линейно независимых собственных векторов $\\boldsymbol x_1, \\ldots, \\boldsymbol x_n$, то матрица $\\boldsymbol X = [\\boldsymbol x_1 \\ldots \\boldsymbol x_n]$ **диагонализирует** матрицу $\\boldsymbol A$:\n",
    "\n",
    "```{math}\n",
    ":label: diagonalize\n",
    "    \\boldsymbol X^{-1} \\boldsymbol A \\boldsymbol X =\n",
    "    \\boldsymbol\\Lambda = \\mathrm{diag} \\{\\lambda_1, \\ldots, \\lambda_n\\}.\n",
    "```\n",
    "\n",
    "```{admonition} Proof\n",
    ":class: dropdown\n",
    "\n",
    "Требуется проверить равенство $\\boldsymbol{AX} = \\boldsymbol {X\\Lambda}$. Имеем\n",
    "\n",
    "$$\n",
    "    \\boldsymbol{AX} = [\\boldsymbol{Ax}_1 \\ldots \\boldsymbol {Ax}_n] = [\\lambda_1\\boldsymbol{x}_1 \\ldots \\lambda_n\\boldsymbol {x}_n].\n",
    "$$\n",
    "\n",
    "С другой стороны,\n",
    "\n",
    "$$\n",
    "    \\boldsymbol{X\\Lambda} = \\boldsymbol X [\\lambda_1 \\boldsymbol e_1 \\ldots \\lambda_n \\boldsymbol e_n] = [\\lambda_1 \\boldsymbol {Xe}_1 \\ldots \\lambda_n \\boldsymbol {Xe}_n] =\n",
    "    [\\lambda_1\\boldsymbol{x}_1 \\ldots \\lambda_n\\boldsymbol {x}_n].\n",
    "$$\n",
    "\n",
    "Итак, $\\boldsymbol{AX} = \\boldsymbol{X\\Lambda}$, или $\\boldsymbol X^{-1} \\boldsymbol A \\boldsymbol X = \\boldsymbol\\Lambda$. Иногда также полезно представление $\\boldsymbol A = \\boldsymbol X \\boldsymbol\\Lambda \\boldsymbol X^{-1}$.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Матрица $\\boldsymbol A$ имеет **простой спектр**, если\n",
    "\n",
    "$$\n",
    "    \\mathrm{spec}(\\boldsymbol A) = \\{\\lambda_1, \\ldots, \\lambda_n\\},\n",
    "$$\n",
    "\n",
    "т.е. $\\mu_{\\boldsymbol A}(\\lambda) = 1$ для всех $\\lambda \\in \\mathrm{spec}(\\boldsymbol A)$.\n",
    "\n",
    "\n",
    "Диагонализируемость матрицы с простым спектром вытекает из следующего утверждения.\n",
    "\n",
    "**Лемма**. Пусть $\\lambda_1, \\ldots, \\lambda_m$ — различные собственные значения матрицы $\\boldsymbol A \\in \\mathbb R^{n\\times n}$, и $\\boldsymbol A\\boldsymbol x_k = \\lambda_k\\boldsymbol x_k$, $k=1,\\ldots, m$. Тогда собственные векторы $\\boldsymbol x_1, \\ldots,\\boldsymbol x_m$ линейно независимы.\n",
    "\n",
    "````{admonition} Proof\n",
    ":class: dropdown\n",
    "Рассуждение проведём индукцией по $m$. При $m=1$ утверждение, очевидно, верно. Пусть теперь $m>1$ и\n",
    "\n",
    "```{math}\n",
    "    :label: linear-dep\n",
    "    c_1\\boldsymbol x_1 + \\ldots + c_{m-1} \\boldsymbol x_{m-1} + c_m\\boldsymbol x_m = \\boldsymbol 0. \n",
    "```\n",
    "\n",
    "Умножим это равенство на матирцу $\\boldsymbol A$ слева:\n",
    "\n",
    "$$\n",
    "    c_1 \\lambda_1\\boldsymbol x_1 + \\ldots + c_{m-1}\\lambda_{m-1}\\boldsymbol x_{m-1}+ c_m\\lambda_m\\boldsymbol x_m = \\boldsymbol 0.\n",
    "$$\n",
    "\n",
    "Вычтем отсюда равенство {eq}`linear-dep`, умноженное на $\\lambda_m$:\n",
    "\n",
    "$$\n",
    "    c_1 (\\lambda_1 - \\lambda_m)\\boldsymbol x_1 + \\ldots + c_{m-1}(\\lambda_{m-1} - \\lambda_m)\\boldsymbol x_{m-1} = \\boldsymbol 0.\n",
    "$$\n",
    "\n",
    "По предположению индукции векторы $\\boldsymbol x_1, \\ldots,\\boldsymbol x_{m-1}$ линейно независимы, поэтому все коэффициенты линейной комбинации из последнего равенства равны нулю:\n",
    "\n",
    "$$\n",
    "    c_k(\\lambda_k - \\lambda_m) = 0, \\quad 1\\leqslant k \\leqslant m-1.\n",
    "$$\n",
    "\n",
    "Все собственные значения различны, значит, $c_k = 0$ при $k=1, \\ldots, m-1$. Но тогда {eq}`linear-dep` превращается в $c_m\\boldsymbol x_m = \\boldsymbol 0$, откуда $c_m = 0$. Итак, все коэффициенты линейной комбинации {eq}`linear-dep` обнулились, поэтому система собственных векторов $\\boldsymbol x_1, \\ldots,\\boldsymbol x_m$ линейно независима.\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Применяя лемму к матрице $\\boldsymbol A \\in \\mathbb R^{n\\times n}$ с простым спектром, заключаем, что у неё есть $n$ линейно независимых собственных векторов $\\boldsymbol x_1, \\ldots,\\boldsymbol x_n$, которые и образуют собственный базис. Следовательно, матрица $\\boldsymbol A$ диагонализируема.\n",
    "\n",
    "Ну а что насчёт диагонализируемости, если спектр не простой? Бывает по-разному. Например, у единичной матрицы спектр отнюдь не простой (все $\\lambda = 1$), однако, она уже диагональна. А вот у матрицы\n",
    "\n",
    "```{math}\n",
    ":label: non-diag\n",
    "\\boldsymbol A =\n",
    "\\begin{pmatrix}\n",
    "        1 & 1 \\\\\n",
    "        0 & 1 \\\\\n",
    "    \\end{pmatrix}\n",
    "```\n",
    "\n",
    "есть одно собственное значение $\\lambda = 1$ кратности $2$, но у него можно найти только один линейно независимый собственный вектор $(1, 0)$. Собственного базиса не построишь, матрица не диагонализируема.\n",
    "\n",
    "## When diagonalization is impossible\n",
    "\n",
    "В общем случае собственный базис и диагонализируемость матрицы $\\boldsymbol A \\in \\mathbb R^{n\\times n}$  могут отсутствовать по двум причинам:\n",
    "\n",
    "1. не хватает собственных значений: \n",
    "\n",
    "    $$\n",
    "    \\sum \\limits_{ \\lambda \\in \\mathrm{spec}(\\boldsymbol A)}\\mu_{\\boldsymbol A}(\\lambda) < n;\n",
    "    $$\n",
    "\n",
    "2. не хватает собственных векторов: \n",
    "\n",
    "    $$\n",
    "    \\gamma_{\\boldsymbol A}(\\lambda) < \\mu_{\\boldsymbol A}(\\lambda) \\text{ для некоторого }\\lambda \\in \\mathrm{spec}(\\boldsymbol A).\n",
    "    $$\n",
    "\n",
    "Первая причина снова приводит нас к вопросу о поле скаляров $F$ векторного пространства. Если $F = \\mathbb C$, то проблем нет: по основной теореме алгебры суммарная алгебраическая кратность собственных чисел всегда равна $n$. А вот дефолтное поле $F=\\mathbb R$ не гарантирует наличие у матрицы полного набора собственных значений из этого поля. Ну а если не хватает собственных значений, то собственных векторов для построения собственного базиса и подавно не хватит.\n",
    "\n",
    "```{admonition} Example: rotation matrix\n",
    ":class: tip, dropdown\n",
    "Снова рассмотрим матрицу поворота\n",
    "\n",
    "$$\n",
    "\\boldsymbol A =\n",
    "    \\begin{pmatrix}\n",
    "        \\cos\\theta & -\\sin\\theta \\\\\n",
    "        \\sin\\theta & \\cos\\theta \\\\\n",
    "    \\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "Поскольку\n",
    "\n",
    "$$\n",
    "    \\chi_{\\boldsymbol A}(\\lambda) = \\begin{vmatrix}\n",
    "        \\cos\\theta - \\lambda & -\\sin\\theta \\\\\n",
    "        \\sin\\theta & \\cos\\theta - \\lambda \\\\\n",
    "    \\end{vmatrix} = \n",
    "    \\lambda^2 - 2\\lambda\\cos\\theta + 1 = 0 \\iff \\lambda = e^{\\pm i\\theta},\n",
    "$$\n",
    "\n",
    "у матрицы $\\boldsymbol A$ нет действительных собственных значений, и про диагонализацию над полем $F = \\mathbb R$ можно забыть. А вот при $F = \\mathbb C$ всё хорошо: находим собственные вектора\n",
    "\n",
    "$$\n",
    "    \\boldsymbol x_1 = \\begin{pmatrix}\n",
    "        i \\\\\n",
    "        1\n",
    "    \\end{pmatrix},\n",
    "    \\quad \n",
    "    \\boldsymbol x_2 = \\begin{pmatrix}\n",
    "        -i \\\\\n",
    "        1\n",
    "    \\end{pmatrix},\n",
    "$$\n",
    "\n",
    "составляем из них матрицу\n",
    "\n",
    "$$\n",
    "\\boldsymbol X = \\begin{pmatrix}\n",
    "        i & -i\\\\\n",
    "        1 & 1\n",
    "    \\end{pmatrix},\n",
    "    \\quad\n",
    "\\boldsymbol X^{-1} = \\frac 12\\begin{pmatrix}\n",
    "        1 & i\\\\\n",
    "        -1 & i\n",
    "    \\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "В результате согласно равенству {eq}`diagonalize` имеем\n",
    "\n",
    "$$\n",
    "    \\frac 12 \\begin{pmatrix}\n",
    "        1 & i\\\\\n",
    "        -1 & i\n",
    "    \\end{pmatrix}\n",
    "    \\begin{pmatrix}\n",
    "        \\cos\\theta & -\\sin\\theta \\\\\n",
    "        \\sin\\theta & \\cos\\theta \\\\\n",
    "    \\end{pmatrix}\n",
    "    \\begin{pmatrix}\n",
    "        i & -i\\\\\n",
    "        1 & 1\n",
    "    \\end{pmatrix}\n",
    "    =\n",
    "    \\begin{pmatrix}\n",
    "        e^{i\\theta} & 0\\\\\n",
    "        0 & e^{-i\\theta}\n",
    "    \\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "Диагонализация матрицы поворота над полем $\\mathbb C$ проведена успешно.\n",
    "```\n",
    "\n",
    "Как видно из предыдущего примера, поле скаляров существенно влияет на принципиальную возможность диагонализации матрицы. По этой причине часто говорят о «диагонализации над полем $F$». По умолчанию подразумевается, что $F = \\mathbb R$, так что просто «диагонализация» — это диагонализация над $\\mathbb R$.\n",
    "\n",
    "Но даже если собственных значений достаточно, возможно другое препятствие: для некоторого собственного значения не хватает линейно независимых собственных векторов. В таком случае собственного базиса также не получится. Простейший пример — матрица {eq}`non-diag`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criterion of diagonalizability\n",
    "\n",
    "Если же у матрицы нет проблем 1 и 2, то она диагонализируема.\n",
    "\n",
    "**Теорема**. Матрица $\\boldsymbol A  \\in \\mathbb R^{n\\times n}$ диагонализируема, если\n",
    "    \n",
    "$$\n",
    "    \\sum \\limits_{ \\lambda \\in \\mathrm{spec}(\\boldsymbol  A)}\\mu_{\\boldsymbol  A}(\\lambda) = n\n",
    "$$\n",
    "\n",
    "и $\\gamma_{\\boldsymbol A}(\\lambda) = \\mu_{\\boldsymbol  A}(\\lambda)$ для всех $\\lambda \\in \\mathrm{spec}(\\boldsymbol A)$.\n",
    "\n",
    "```{admonition} Proof\n",
    ":class: dropdown\n",
    "Пусть $\\lambda_1, \\ldots, \\lambda_m$ — все различные собственные значения матрицы $\\boldsymbol A$, имеющие кратности \n",
    "\n",
    "$$\n",
    "\\mu_{\\boldsymbol A}(\\lambda_i)=\\gamma_{\\boldsymbol A}(\\lambda_i) = k_i,\\quad\n",
    "\\sum\\limits_{i=1}^m k_i = n.\n",
    "$$\n",
    "\n",
    "Обозначим $V^\\lambda = N(\\boldsymbol A - \\lambda \\mathcal I)$, тогда $\\dim V^{\\lambda_i} = k_i$. В силу вышеприведённой леммы любой набор собственных векторов, отвечающих различным собственным значениям, линейно независим. Следовательно,\n",
    "\n",
    "$$\n",
    "    V^{\\lambda_i} \\cap \\big(V^{\\lambda_1} + \\ldots +V^{\\lambda_{i-1}}+V^{\\lambda_{i-1}} + V^{\\lambda_m}\\big) = \\{\\boldsymbol 0\\},\n",
    "$$\n",
    "\n",
    "и поэтому сумма подпространств $V^{\\lambda_i}$ прямая,\n",
    "\n",
    "$$\n",
    "    V^{\\lambda_1} \\oplus \\ldots \\oplus V^{\\lambda_m} = V,\n",
    "$$\n",
    "\n",
    "а поскольку $\\dim V = \\sum\\limits_{i=1}^m k_i = n$, то $V = \\mathbb R^n$. Выбирая базис в каждом пространстве $V^{\\lambda_i}$, получаем, что объединение этих базиов даёт базис всего пространства $\\mathbb R^n$. Таким образом, это и есть искомый собственный базис матрицы $\\boldsymbol A$, гарантирующий её диагонализируемость.\n",
    "```\n",
    "\n",
    "Обратное утверждение тоже верно, поэтому данная теорема представляет собой критерий диагонализируемости матрицы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Spectral theorem\n",
    "\n",
    "Если матрица симметричная, то все проблемы с комплексными собственными значениями улетучиваются.\n",
    "\n",
    "**Теорема**. Если $\\boldsymbol A^\\top = \\boldsymbol A$ и $\\lambda\\in\\mathrm{spec}(\\boldsymbol A)$, то $\\lambda \\in \\mathbb R$.\n",
    "\n",
    "```{admonition} Proof\n",
    ":class: dropdown\n",
    "Если $\\boldsymbol{Ax} = \\lambda \\boldsymbol x$, то\n",
    "\n",
    "$$\n",
    "    \\overline{\\boldsymbol x}^\\top \\boldsymbol{Ax} = \\overline{\\boldsymbol x}^\\top \\lambda\\boldsymbol{x} = \\lambda \\overline{\\boldsymbol x}^\\top \\boldsymbol{x}.\n",
    " $$\n",
    "\n",
    "Транспонируя левую часть, получаем\n",
    "\n",
    "$$\n",
    "    (\\overline{\\boldsymbol x}^\\top \\boldsymbol{Ax})^\\top = \\boldsymbol{x}^\\top \\boldsymbol A \\overline{\\boldsymbol x} = \\boldsymbol{x}^\\top \\overline{\\boldsymbol{Ax}} = \\overline \\lambda \\boldsymbol{x}^\\top \\overline{\\boldsymbol x}.\n",
    "$$\n",
    "\n",
    "Транспонирование не меняет значения скаляра, поэтому правые части обеих равнеств одинаковы. Поскольку\n",
    "    \n",
    "$$\n",
    "    \\overline{\\boldsymbol x}^\\top \\boldsymbol{x} = \\boldsymbol{x}^\\top \\overline{\\boldsymbol x} = \\sum\\limits_{k=1}^n \\vert x_k\\vert^2  > 0,\n",
    "$$\n",
    "    \n",
    "получается, что $\\lambda = \\overline \\lambda$.\n",
    "```\n",
    "\n",
    "Итак, всякая симметричная матрица имеет $n$ действительных собственных значений (с учётом кратности). Этим замечательные свойства собственных чисел и векторов таких матриц не исчерпываются. Оказывается, что любые два собственных вектора симметричной матрицы, отвечающие разным собственным значениям, ортогональны.\n",
    "\n",
    "**Теорема**. Если $\\boldsymbol A^\\top = \\boldsymbol A$, $\\boldsymbol {Ax} = \\lambda \\boldsymbol x$, $\\boldsymbol {Ay} = \\mu \\boldsymbol y$, $\\lambda \\ne \\mu$, то $\\boldsymbol x^\\top\\boldsymbol y = 0$.\n",
    "\n",
    "```{admonition} Proof\n",
    ":class: dropdown\n",
    "Имеем\n",
    "\n",
    "$$\n",
    "    (\\lambda\\boldsymbol x)^\\top \\boldsymbol y = (\\boldsymbol{Ax})^\\top\\boldsymbol y =\n",
    "    \\boldsymbol x^\\top \\boldsymbol A^\\top \\boldsymbol y  = \\boldsymbol x^\\top (\\boldsymbol A \\boldsymbol y) = \\boldsymbol x^\\top \\mu \\boldsymbol y.\n",
    "$$\n",
    "\n",
    "Поскольку $\\lambda \\ne \\mu$, отсюда следует, что $\\boldsymbol x^\\top \\boldsymbol y = 0$, т.е. собственные векторы ортогональны.\n",
    "```\n",
    "\n",
    "Как мы помним, матрица с простым спектром всегда может быть диагонализирована. Из предыдущей теоремы следует, что если матрица вдобавок ещё и симметрична, то у неё  существует ортнонормированный собственный базис.\n",
    "\n",
    "````{prf:theorem} spectral\n",
    ":label: spectral\n",
    ":nonumber:\n",
    "Если симметричная матрица $\\boldsymbol A$ имеет простой спектр \n",
    "\n",
    "$$\n",
    "\\mathrm{spec}(\\boldsymbol A) = \\{\\lambda_1, \\ldots, \\lambda_n\\},\n",
    "$$\n",
    "\n",
    "то существует такая ортогональная матрица $\\boldsymbol Q$, что\n",
    "\n",
    "$$\n",
    "    \\boldsymbol Q^\\top\\boldsymbol{AQ} = \\boldsymbol \\Lambda = \\mathrm{diag}\\{\\lambda_1, \\ldots, \\lambda_n\\}.\n",
    "$$\n",
    "````\n",
    "\n",
    "Верна ли спектральная теорема для всех симметричных матриц, не обязательно с простым спектром? Оказывается, да! Один из способов это доказать — воспользоваться **разложением Шура**.\n",
    "\n",
    "**Теорема** (разложение Шура). Всякая квадратная матрица $\\boldsymbol A$ может быть представлена в виде \n",
    "\n",
    "```{math} \n",
    ":label: schur-decomp\n",
    "    \\boldsymbol A = \\boldsymbol{QU}\\boldsymbol Q^{-1},\n",
    "```\n",
    "\n",
    "где матрица $\\boldsymbol Q$ унитарная, а матрица $\\boldsymbol U$ — верхняя треугольная. \n",
    "\n",
    "Если все собственные значения матрицы $\\boldsymbol A$ действительные, то все матрицы в разложении {eq}`schur-decomp` можно выбрать вещественными, и тогда матрица $\\boldsymbol Q$ ортогональная.\n",
    "\n",
    "Если $\\boldsymbol A = \\boldsymbol A^\\top$, то из раложения {eq}`schur-decomp` получаем\n",
    "\n",
    "$$\n",
    "    \\boldsymbol U = \\boldsymbol Q^\\top \\boldsymbol{AQ} \\Rightarrow\n",
    "    \\boldsymbol U^\\top = \\boldsymbol{Q}^\\top\\boldsymbol {AQ} = \\boldsymbol U^\\top.\n",
    "$$\n",
    "\n",
    "Таким образом, верхняя треугольная матрица $\\boldsymbol U$ симметрична и, стало быть, диагональна. Значит, разложение {eq}`schur-decomp` и есть диагонализация симметричной матрицы $\\boldsymbol A$."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
