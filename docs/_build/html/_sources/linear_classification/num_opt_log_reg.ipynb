{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numeric optimization\n",
    "\n",
    "To find the optimal weights of the logistic regression, we can use {prf:ref}`gradient descent <GD>` algorithm. To apply this algorithm, one need to calculate the gradient of the loss function.\n",
    "\n",
    "## Binary logistic regression\n",
    "\n",
    "Multiply the loss function {eq}`bin-log-reg-loss` by $n$:\n",
    "\n",
    "$$\n",
    "\\mathcal L(\\boldsymbol w) = \n",
    "-\\sum\\limits_{i=1}^n \\big(y_i \\log(\\sigma(\\boldsymbol x_i^\\top \\boldsymbol w)) + (1- y_i)\\log(1 - \\sigma(\\boldsymbol x_i^\\top \\boldsymbol w))\\big).\n",
    "$$\n",
    "\n",
    "To find $\\nabla \\mathcal L(\\boldsymbol w)$ observe that\n",
    "\n",
    "$$\n",
    "   \\nabla \\log(\\sigma(\\boldsymbol x_i^\\top \\boldsymbol w)) = \\frac {\\nabla \\sigma(\\boldsymbol x_i^\\top \\boldsymbol w)}{\\sigma(\\boldsymbol x_i^\\top \\boldsymbol w)} = \n",
    "   \\frac{\\sigma'(\\boldsymbol x_i^\\top \\boldsymbol w) \\nabla(\\boldsymbol x_i^\\top \\boldsymbol w)}{{\\sigma(\\boldsymbol x_i^\\top \\boldsymbol w)}}.\n",
    "$$\n",
    "\n",
    "Also,\n",
    "\n",
    "$$\n",
    "   \\nabla \\log(1 - \\sigma(\\boldsymbol x_i^\\top \\boldsymbol w)) = -\\frac {\\nabla  \\sigma(\\boldsymbol x_i^\\top \\boldsymbol w)}{1 - \\sigma(\\boldsymbol x_i^\\top \\boldsymbol w)} = \n",
    "   \\frac{\\sigma'(\\boldsymbol x_i^\\top \\boldsymbol w) \\nabla(\\boldsymbol x_i^\\top \\boldsymbol w)}{{1 - \\sigma(\\boldsymbol x_i^\\top \\boldsymbol w)}}.\n",
    "$$\n",
    "\n",
    "**Q**. What is $\\nabla(\\boldsymbol x_i^\\top \\boldsymbol w)$?\n",
    "\n",
    "Putting it altogeter, we get\n",
    "\n",
    "$$\n",
    "   \\nabla \\mathcal L(\\boldsymbol w) = -\\sum\\limits_{i=1}^n \\big(y_i(1 - \\sigma(\\boldsymbol x_i^\\top \\boldsymbol w))\\boldsymbol x_i - (1-y_i)\\sigma(\\boldsymbol x_i^\\top \\boldsymbol w)\\boldsymbol x_i\\big) = \\sum\\limits_{i=1}^n (\\sigma(\\boldsymbol x_i^\\top \\boldsymbol w) - y_i)\\boldsymbol x_i.\n",
    "$$\n",
    "\n",
    "````{admonition} Question\n",
    ":class: important\n",
    "How to write $\\nabla \\mathcal L(\\boldsymbol w)$ as a product of a matrix and a vector, avoiding the explicit summation?\n",
    "\n",
    "```{hint}\n",
    ":class: dropdown\n",
    "The shape of $\\nabla \\mathcal L(\\boldsymbol w)$ is the same as of $\\boldsymbol w$, i.e., $d\\times 1$. Now observe that\n",
    "\n",
    "$$\n",
    "   \\begin{pmatrix}\n",
    "   \\sigma(\\boldsymbol x_1^\\top \\boldsymbol w) - y_1 \\\\\n",
    "   \\vdots \\\\\n",
    "   \\sigma(\\boldsymbol x_n^\\top \\boldsymbol w) - y_n\n",
    "   \\end{pmatrix}\n",
    "   = \\sigma(\\boldsymbol X \\boldsymbol w )- \\boldsymbol y \\in \\mathbb R^n.\n",
    "$$\n",
    "\n",
    "What should we multiply by this vector to obtain $\\nabla \\mathcal L$?\n",
    "```\n",
    "````\n",
    "\n",
    "````{admonition} Question\n",
    ":class: important\n",
    " What is hessian $\\nabla^2 L(\\boldsymbol w)$?\n",
    "\n",
    "```{admonition} Answer\n",
    ":class: tip, dropdown\n",
    "$$\n",
    "\\nabla^2 L(\\boldsymbol w) = \\boldsymbol X^\\top \\boldsymbol S \\boldsymbol X,\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "   \\boldsymbol S = \\mathrm{diag}\\{\\sigma(\\boldsymbol X \\boldsymbol w )- \\boldsymbol y\\} = \\begin{pmatrix}\n",
    "   \\sigma(\\boldsymbol x_1^{\\boldsymbol{\\top}} \\boldsymbol w) - y_1  & \\ldots & 0 \\\\\n",
    "   \\vdots & \\ddots & \\vdots \\\\\n",
    "   0 & \\ldots & \\sigma(\\boldsymbol x_n^{\\boldsymbol{\\top}} \\boldsymbol w) - y_n\n",
    "   \\end{pmatrix}\n",
    "$$\n",
    "```\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breast cancer dataset: numeric optimization \n",
    "\n",
    "Fetch the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((569, 30), (569,))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the {prf:ref}`gradient descent <GD>` algorithm to the logistic regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "\n",
    "def logistic_regression_gd(X, y, C=1, learning_rate=0.01, tol=1e-3, max_iter=10000):\n",
    "    w = np.random.normal(size=X.shape[1])\n",
    "    gradient = X.T.dot(expit(X.dot(w)) - y) + C * w\n",
    "    for i in range(max_iter):\n",
    "        if np.linalg.norm(gradient) <= tol:\n",
    "            return w\n",
    "        w -= learning_rate * gradient\n",
    "        gradient = X.T.dot(X.dot(w) - y) + C * w\n",
    "    print(\"max_iter exceeded\")\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the logistic regresion on the whole dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_iter exceeded\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 1.50645717,  0.88616411,  0.23284509, -0.03134607, -0.11193806,\n",
       "        0.75854782, -1.13578016,  0.25471101, -0.27842214, -0.08007865,\n",
       "       -0.12617765, -1.63273572, -0.28655052,  0.021978  ,  0.34426614,\n",
       "        1.46758903,  2.19617047, -0.51490271,  0.00296075, -1.10648936,\n",
       "        0.83226328, -0.74504871, -0.44063441,  0.01737396,  0.06614479,\n",
       "        0.70875487,  1.09165239,  0.07451502,  2.05160578, -1.24672935])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = logistic_regression_gd(X, y, learning_rate=1e-9, max_iter=10**5)\n",
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the accuracy score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6731107205623902"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(expit(X.dot(w)) > 0.5, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare with `sklearn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9595782073813708\n",
      "0.9595782073813708\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 2.18332329,  0.11789056, -0.0696952 , -0.00350176, -0.16519865,\n",
       "        -0.4118028 , -0.67385012, -0.36497435, -0.23983217, -0.02406538,\n",
       "        -0.02186803,  1.20613335,  0.04838948, -0.0970984 , -0.01870574,\n",
       "         0.01626219, -0.03861864, -0.04276028, -0.04322983,  0.00846606,\n",
       "         1.29672397, -0.34212421, -0.12576754, -0.02460068, -0.30612789,\n",
       "        -1.14244163, -1.62895872, -0.6992171 , -0.72520831, -0.11275416]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_reg = LogisticRegression(fit_intercept=False, max_iter=5000)\n",
    "log_reg.fit(X, y)\n",
    "print(log_reg.score(X, y))\n",
    "print(accuracy_score(log_reg.predict(X), y))\n",
    "log_reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.407318017237933"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(w - log_reg.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial logistic regression\n",
    "\n",
    "Recall that the loss function in this case is\n",
    "\n",
    "$$\n",
    "    \\begin{multline*}\n",
    "    \\mathcal L(\\boldsymbol W) = -\\sum\\limits_{i=1}^n \\sum\\limits_{k=1}^K  y_{ik} \\bigg(\\boldsymbol x_i^\\top\\boldsymbol w_{k} -\\log\\Big(\\sum\\limits_{k=1}^K \\exp(\\boldsymbol x_i^\\top\\boldsymbol w_{k})\\Big)\\bigg) = \\\\\n",
    "    =\n",
    "    -\\sum\\limits_{i=1}^n \\sum\\limits_{k=1}^K  y_{ik} \\bigg(\\sum\\limits_{j=1}^d x_{ij} w_{jk} -\\log\\Big(\\sum\\limits_{k=1}^K \\exp\\Big(\\sum\\limits_{j=1}^d x_{ij} w_{jk}\\Big)\\Big)\\bigg)\n",
    "    \\end{multline*}\n",
    "$$\n",
    "\n",
    "One can show that \n",
    "\n",
    "$$\n",
    "    \\nabla \\mathcal L(\\boldsymbol W) = \\boldsymbol X^\\top (\\boldsymbol {\\widehat Y} - \\boldsymbol Y) = \\boldsymbol X^\\top ( \\sigma(\\boldsymbol{XW}) - \\boldsymbol Y).\n",
    "$$\n",
    "\n",
    "<!-- Observe that\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial}{\\partial w_{pq}} (x_{ij} w_{jk}) = x_{ij} \\delta_{pj} \\delta_{qk},\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial w_{pq}}\\bigg(\\log\\Big(\\sum\\limits_{k=1}^K \\exp\\Big(\\sum\\limits_{j=1}^d x_{ij} w_{jk}\\Big)\\Big)\\bigg) = \\frac{\\exp\\Big(\\sum\\limits_{j=1}^d x_{ij} w_{jk}\\Big)}{\\sum\\limits_{k=1}^K \\exp\\Big(\\sum\\limits_{j=1}^d x_{ij} w_{jk}\\Big)} x_{ip} \\delta_{qk}\n",
    "$$\n",
    "\n",
    "Hence, \n",
    "\n",
    "$$\n",
    "    \\frac{\\partial \\mathcal L}{\\partial w_{pq}} = \\sum\\limits_{i=1}^n \\sum\\limits_{k=1}^K y_{ik}\\bigg(\\sum\\limits_{j=1}^d  \\bigg(  x_{ij} \\delta_{pj} \\delta_{qk} - \\frac{\\exp\\Big(\\sum\\limits_{j=1}^d x_{ij} w_{jk}\\Big)}{\\sum\\limits_{k=1}^K \\exp\\Big(\\sum\\limits_{j=1}^d x_{ij} w_{jk}\\Big)} x_{ip} \\delta_{qk}\\bigg)\\bigg)\n",
    "$$ -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} TODO\n",
    ":class: warning\n",
    "* Try numerical optimization on several datasets\n",
    "* Apply Newton's method and compare it's performance with GD\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
