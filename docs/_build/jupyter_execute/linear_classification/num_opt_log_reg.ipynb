{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numeric optimization\n",
    "\n",
    "To find the optimal weights of the logistic regression, we can use {prf:ref}`gradient descent <GD>` algorithm. To apply this algorithm, one need to calculate the gradient of the loss function.\n",
    "\n",
    "## Binary logistic regression\n",
    "\n",
    "Multiply the loss function {eq}`bin-log-reg-loss` by $n$:\n",
    "\n",
    "$$\n",
    "\\mathcal L(\\boldsymbol w) = \n",
    "-\\sum\\limits_{i=1}^n \\big(y_i \\log(\\sigma(\\boldsymbol x_i^\\top \\boldsymbol w)) + (1- y_i)\\log(1 - \\sigma(\\boldsymbol x_i^\\top \\boldsymbol w))\\big).\n",
    "$$\n",
    "\n",
    "To find $\\nabla \\mathcal L(\\boldsymbol w)$ observe that\n",
    "\n",
    "$$\n",
    "   \\nabla \\log(\\sigma(\\boldsymbol x_i^\\top \\boldsymbol w)) = \\frac {\\nabla \\sigma(\\boldsymbol x_i^\\top \\boldsymbol w)}{\\sigma(\\boldsymbol x_i^\\top \\boldsymbol w)} = \n",
    "   \\frac{\\sigma'(\\boldsymbol x_i^\\top \\boldsymbol w) \\nabla(\\boldsymbol x_i^\\top \\boldsymbol w)}{{\\sigma(\\boldsymbol x_i^\\top \\boldsymbol w)}}.\n",
    "$$\n",
    "\n",
    "Also,\n",
    "\n",
    "$$\n",
    "   \\nabla \\log(1 - \\sigma(\\boldsymbol x_i^\\top \\boldsymbol w)) = -\\frac {\\nabla  \\sigma(\\boldsymbol x_i^\\top \\boldsymbol w)}{1 - \\sigma(\\boldsymbol x_i^\\top \\boldsymbol w)} = \n",
    "   \\frac{\\sigma'(\\boldsymbol x_i^\\top \\boldsymbol w) \\nabla(\\boldsymbol x_i^\\top \\boldsymbol w)}{{1 - \\sigma(\\boldsymbol x_i^\\top \\boldsymbol w)}}.\n",
    "$$\n",
    "\n",
    "**Q**. What is $\\nabla(\\boldsymbol x_i^\\top \\boldsymbol w)$?\n",
    "\n",
    "Putting it altogeter, we get\n",
    "\n",
    "$$\n",
    "   \\nabla \\mathcal L(\\boldsymbol w) = -\\sum\\limits_{i=1}^n \\big(y_i(1 - \\sigma(\\boldsymbol x_i^\\top \\boldsymbol w))\\boldsymbol x_i - (1-y_i)\\sigma(\\boldsymbol x_i^\\top \\boldsymbol w)\\boldsymbol x_i\\big) = \\sum\\limits_{i=1}^n (\\sigma(\\boldsymbol x_i^\\top \\boldsymbol w) - y_i)\\boldsymbol x_i.\n",
    "$$\n",
    "\n",
    "````{admonition} Question\n",
    ":class: important\n",
    "How to write $\\nabla \\mathcal L(\\boldsymbol w)$ as a product of a matrix and a vector, avoiding the explicit summation?\n",
    "\n",
    "```{hint}\n",
    ":class: dropdown\n",
    "The shape of $\\nabla \\mathcal L(\\boldsymbol w)$ is the same as of $\\boldsymbol w$, i.e., $d\\times 1$. Now observe that\n",
    "\n",
    "$$\n",
    "   \\begin{pmatrix}\n",
    "   \\sigma(\\boldsymbol x_1^\\top \\boldsymbol w) - y_1 \\\\\n",
    "   \\vdots \\\\\n",
    "   \\sigma(\\boldsymbol x_n^\\top \\boldsymbol w) - y_n\n",
    "   \\end{pmatrix}\n",
    "   = \\sigma(\\boldsymbol X \\boldsymbol w )- \\boldsymbol y \\in \\mathbb R^n.\n",
    "$$\n",
    "\n",
    "What should we multiply by this vector to obtain $\\nabla \\mathcal L$?\n",
    "```\n",
    "````\n",
    "\n",
    "````{admonition} Question\n",
    ":class: important\n",
    " What is hessian $\\nabla^2 L(\\boldsymbol w)$?\n",
    "\n",
    "```{admonition} Answer\n",
    ":class: tip, dropdown\n",
    "$$\n",
    "\\nabla^2 L(\\boldsymbol w) = \\boldsymbol X^\\top \\boldsymbol S \\boldsymbol X,\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "   \\boldsymbol S = \\mathrm{diag}\\{\\sigma(\\boldsymbol X \\boldsymbol w )- \\boldsymbol y\\} = \\begin{pmatrix}\n",
    "   \\sigma(\\boldsymbol x_1^{\\boldsymbol{\\top}} \\boldsymbol w) - y_1  & \\ldots & 0 \\\\\n",
    "   \\vdots & \\ddots & \\vdots \\\\\n",
    "   0 & \\ldots & \\sigma(\\boldsymbol x_n^{\\boldsymbol{\\top}} \\boldsymbol w) - y_n\n",
    "   \\end{pmatrix}\n",
    "$$\n",
    "```\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breast cancer dataset: numeric optimization \n",
    "\n",
    "Fetch the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((569, 30), (569,))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the {prf:ref}`gradient descent <GD>` algorithm to the logistic regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "\n",
    "def logistic_regression_gd(X, y, C=1, learning_rate=0.01, tol=1e-3, max_iter=10000):\n",
    "    w = np.random.normal(size=X.shape[1])\n",
    "    gradient = X.T.dot(expit(X.dot(w)) - y) + C * w\n",
    "    for i in range(max_iter):\n",
    "        if np.linalg.norm(gradient) <= tol:\n",
    "            return w\n",
    "        w -= learning_rate * gradient\n",
    "        gradient = X.T.dot(X.dot(w) - y) + C * w\n",
    "    print(\"max_iter exceeded\")\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the logistic regresion on the whole dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_iter exceeded\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.95916668, -0.54046488, -0.49365735,  0.01951272,  1.16513025,\n",
       "       -0.92025639, -0.35628036,  0.07792495, -0.08304842, -0.5860345 ,\n",
       "        2.35522508, -0.6339002 ,  0.96174227, -0.04107821, -1.56653769,\n",
       "       -0.12061115, -0.4241051 ,  0.29403223, -0.54488831, -0.87231864,\n",
       "        0.20096651,  0.31404291,  0.31351565, -0.01779842, -1.00909062,\n",
       "       -1.71039148, -0.00979913,  1.43453601, -0.38925477,  0.95908276])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = logistic_regression_gd(X, y, learning_rate=1e-9, max_iter=10**5)\n",
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the accuracy score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5026362038664324"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(expit(X.dot(w)) > 0.5, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare with `sklearn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9595782073813708\n",
      "0.9595782073813708\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 2.1975179 ,  0.11135549, -0.07305484, -0.00335447, -0.16745265,\n",
       "        -0.41425964, -0.67991182, -0.36939821, -0.2430434 , -0.02410403,\n",
       "        -0.02336256,  1.20717466,  0.04631705, -0.09649416, -0.01903567,\n",
       "         0.01810195, -0.03747183, -0.04324005, -0.04367351,  0.00878879,\n",
       "         1.29284173, -0.33773315, -0.12388807, -0.02474166, -0.31037867,\n",
       "        -1.14588466, -1.63908301, -0.70741377, -0.73405013, -0.11303725]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_reg = LogisticRegression(fit_intercept=False, max_iter=5000)\n",
    "log_reg.fit(X, y)\n",
    "print(log_reg.score(X, y))\n",
    "print(accuracy_score(log_reg.predict(X), y))\n",
    "log_reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.4344292111849075"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(w - log_reg.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial logistic regression\n",
    "\n",
    "Recall that the loss function in this case is\n",
    "\n",
    "$$\n",
    "    \\begin{multline*}\n",
    "    \\mathcal L(\\boldsymbol W) = -\\sum\\limits_{i=1}^n \\sum\\limits_{k=1}^K  y_{ik} \\bigg(\\boldsymbol x_i^\\top\\boldsymbol w_{k} -\\log\\Big(\\sum\\limits_{k=1}^K \\exp(\\boldsymbol x_i^\\top\\boldsymbol w_{k})\\Big)\\bigg) = \\\\\n",
    "    =\n",
    "    -\\sum\\limits_{i=1}^n \\sum\\limits_{k=1}^K  y_{ik} \\bigg(\\sum\\limits_{j=1}^d x_{ij} w_{jk} -\\log\\Big(\\sum\\limits_{k=1}^K \\exp\\Big(\\sum\\limits_{j=1}^d x_{ij} w_{jk}\\Big)\\Big)\\bigg)\n",
    "    \\end{multline*}\n",
    "$$\n",
    "\n",
    "One can show that \n",
    "\n",
    "$$\n",
    "    \\nabla \\mathcal L(\\boldsymbol W) = \\boldsymbol X^\\top (\\boldsymbol {\\widehat Y} - \\boldsymbol Y) = \\boldsymbol X^\\top ( \\sigma(\\boldsymbol{XW}) - \\boldsymbol Y).\n",
    "$$\n",
    "\n",
    "<!-- Observe that\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial}{\\partial w_{pq}} (x_{ij} w_{jk}) = x_{ij} \\delta_{pj} \\delta_{qk},\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial w_{pq}}\\bigg(\\log\\Big(\\sum\\limits_{k=1}^K \\exp\\Big(\\sum\\limits_{j=1}^d x_{ij} w_{jk}\\Big)\\Big)\\bigg) = \\frac{\\exp\\Big(\\sum\\limits_{j=1}^d x_{ij} w_{jk}\\Big)}{\\sum\\limits_{k=1}^K \\exp\\Big(\\sum\\limits_{j=1}^d x_{ij} w_{jk}\\Big)} x_{ip} \\delta_{qk}\n",
    "$$\n",
    "\n",
    "Hence, \n",
    "\n",
    "$$\n",
    "    \\frac{\\partial \\mathcal L}{\\partial w_{pq}} = \\sum\\limits_{i=1}^n \\sum\\limits_{k=1}^K y_{ik}\\bigg(\\sum\\limits_{j=1}^d  \\bigg(  x_{ij} \\delta_{pj} \\delta_{qk} - \\frac{\\exp\\Big(\\sum\\limits_{j=1}^d x_{ij} w_{jk}\\Big)}{\\sum\\limits_{k=1}^K \\exp\\Big(\\sum\\limits_{j=1}^d x_{ij} w_{jk}\\Big)} x_{ip} \\delta_{qk}\\bigg)\\bigg)\n",
    "$$ -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} TODO\n",
    ":class: warning\n",
    "* Try numerical optimization on several datasets\n",
    "* Apply Newton's method and compare it's performance with GD\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}